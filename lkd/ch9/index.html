<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://notes.shichao.io/lkd/ch9/">
        <link rel="shortcut icon" href="../../toki_32.png">
        

	<title>Chapter 9. An Introduction to Kernel Synchronization - Shichao's Notes</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,400italic,500,600" rel="stylesheet">
        <link href="../../custom.css" rel="stylesheet">
        <link href="../../friendly.css" rel="stylesheet">
        <link href="../../theme.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">Shichao's Notes</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">APUE <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../apue/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch1/">Chapter 1. UNIX System Overview</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch2/">Chapter 2. UNIX Standardization and Implementations</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch3/">Chapter 3. File I/O</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch4/">Chapter 4. Files and Directories</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch5/">Chapter 5. Standard I/O Library</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch6/">Chapter 6. System Data Files and Information</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch7/">Chapter 7. Process Environment</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch8/">Chapter 8. Process Control</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch9/">Chapter 9. Process Relationships</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch10/">Chapter 10. Signals</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch11/">Chapter 11. Threads</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch12/">Chapter 12. Thread Control</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch13/">Chapter 13. Daemon Processes</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch14/">Chapter 14. Advanced I/O</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch15/">Chapter 15. Interprocess Communication</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch16/">Chapter 16. Network IPC: Sockets</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch17/">Chapter 17. Advanced IPC</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">LKD <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../ch1/">Chapter 1. Introduction to the Linux Kernel</a>
                        </li>
                      
                        <li>
                            <a href="../ch2/">Chapter 2. Getting Started with the Kernel</a>
                        </li>
                      
                        <li>
                            <a href="../ch3/">Chapter 3. Process Management</a>
                        </li>
                      
                        <li>
                            <a href="../ch4/">Chapter 4. Process Scheduling</a>
                        </li>
                      
                        <li>
                            <a href="../ch5/">Chapter 5. System Calls</a>
                        </li>
                      
                        <li>
                            <a href="../ch6/">Chapter 6. Kernel Data Structures</a>
                        </li>
                      
                        <li>
                            <a href="../ch7/">Chapter 7. Interrupts and Interrupt Handlers</a>
                        </li>
                      
                        <li>
                            <a href="../ch8/">Chapter 8. Bottom Halves and Deferring Work</a>
                        </li>
                      
                        <li class="active">
                            <a href="./">Chapter 9. An Introduction to Kernel Synchronization</a>
                        </li>
                      
                        <li>
                            <a href="../ch10/">Chapter 10. Kernel Synchronization Methods</a>
                        </li>
                      
                        <li>
                            <a href="../ch11/">Chapter 11. Timers and Time Management</a>
                        </li>
                      
                        <li>
                            <a href="../ch12/">Chapter 12. Memory Management</a>
                        </li>
                      
                        <li>
                            <a href="../ch13/">Chapter 13. The Virtual Filesystem</a>
                        </li>
                      
                        <li>
                            <a href="../ch14/">Chapter 14. The Block I/O Layer</a>
                        </li>
                      
                        <li>
                            <a href="../ch15/">Chapter 15. The Process Address Space</a>
                        </li>
                      
                        <li>
                            <a href="../ch16/">Chapter 16. The Page Cache and Page Writeback</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">UNP <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../unp/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch2/">Chapter 2. The Transport Layer: TCP, UDP, and SCTP</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch3/">Chapter 3. Sockets Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch4/">Chapter 4. Elementary TCP Sockets</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch5/">Chapter 5. TCP Client/Server Example</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch6/">Chapter 6. I/O Multiplexing: The select and poll Functions</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch7/">Chapter 7. Socket Options</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch8/">Chapter 8. Elementary UDP Sockets</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">TCPv1 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../tcpv1/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch2/">Chapter 2. The Internet Address Architecture</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch3/">Chapter 3. Link Layer</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch4/">Chapter 4. ARP: Address Resolution Protocol</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch5/">Chapter 5. The Internet Protocol (IP)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch6/">Chapter 6. System Configuration: DHCP and Autoconfiguration</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch7/">Chapter 7. Firewalls and Network Address Translation (NAT)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch8/">Chapter 8. ICMPv4 and ICMPv6: Internet Control Message Protocol</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch9/">Chapter 9. Broadcasting and Local Multicasting (IGMP and MLD)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch10/">Chapter 10. User Datagram Protocol (UDP) and IP Fragmentation</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch11/">Chapter 11. Name Resolution and the Domain Name System (DNS)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch12/">Chapter 12. TCP: The Transmission Control Protocol (Preliminaries)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch13/">Chapter 13. TCP Connection Management</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch14/">Chapter 14. TCP Timeout and Retransmission</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch15/">Chapter 15. TCP Data Flow and Window Management</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch16/">Chapter 16. TCP Congestion Control</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch17/">Chapter 17. TCP Keepalive</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch18/">Chapter 18. Security: EAP, IPsec, TLS, DNSSEC, and DKIM</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/headers/">Headers</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">GOPL <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../gopl/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch1/">Chapter 1. Tutorial</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch2/">Chapter 2. Program Structure</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch3/">Chapter 3. Basic Data Types</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch4/">Chapter 4. Composite Types</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch5/">Chapter 5. Functions</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch6/">Chapter 6. Methods</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch7/">Chapter 7. Interfaces</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch8/">Chapter 8. Goroutines and Channels</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch9/">Chapter 9. Concurrency with Shared Variables</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch10/">Chapter 10. Packages and the Go Tool</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">CSN <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../csn/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../csn/part1/">Part 1: Language</a>
                        </li>
                      
                        <li>
                            <a href="../../csn/part2/">Part 2: Advanced</a>
                        </li>
                      
                    </ul>
                </li>
            <li>
                    <a href="../../toc/">TOC</a>
                </li>
            </ul>
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    
                        <a href="https://github.com/shichao-an/notes/blob/master/docs/lkd/ch9.md">
                    
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#chapter-9-an-introduction-to-kernel-synchronization">Chapter 9. An Introduction to Kernel Synchronization</a></li>
        
    
        <li class="main "><a href="#critical-regions-and-race-conditions">Critical Regions and Race Conditions</a></li>
        
            <li><a href="#why-do-we-need-protection">Why Do We Need Protection?</a></li>
        
            <li><a href="#the-single-variable">The Single Variable</a></li>
        
    
        <li class="main "><a href="#locking">Locking</a></li>
        
            <li><a href="#causes-of-concurrency">Causes of Concurrency</a></li>
        
            <li><a href="#knowing-what-to-protect">Knowing What to Protect</a></li>
        
    
        <li class="main "><a href="#doubts-and-solution">Doubts and Solution</a></li>
        
            <li><a href="#verbatim">Verbatim</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">
              

<h3 id="chapter-9-an-introduction-to-kernel-synchronization"><strong>Chapter 9. An Introduction to Kernel Synchronization</strong></h3>
<p>In a shared memory application, developers must ensure that shared resources are protected from concurrent access. The kernel is no exception. Shared resources require protection from concurrent access because if multiple threads of execution access and manipulate the data at the same time, the threads may overwrite each other's changes or access data while it is in an inconsistent state. Concurrent access of shared data often results in instability is hard to track down and debug.</p>
<p>The term <em>threads of execution</em> implies any instance of executing code. For example, this includes any of the following:</p>
<ul>
<li>A task in the kernel</li>
<li>An interrupt handler</li>
<li>A bottom half</li>
<li>A kernel thread</li>
</ul>
<p>This chapter may shorten <em>threads of execution</em> to simply <em>threads</em>. Keep in mind that this term describes any executing code.</p>
<p>[p161]</p>
<p><a href="https://en.wikipedia.org/wiki/Symmetric_multiprocessing">Symmetrical multiprocessing</a> support was introduced in the 2.0 kernel. Multiprocessing support implies that kernel code can simultaneously run on two or more processors. Consequently, without protection, code in the kernel, running on two different processors, can simultaneously access shared data at exactly the same time. With the introduction of the 2.6 kernel, the Linux kernel is preemptive. This implies that (in the absence of protection) the scheduler can preempt kernel code at virtually any point and reschedule another task. Today, a number of scenarios enable for concurrency inside the kernel, and they all require protection.</p>
<p>This chapter discusses the issues of concurrency and synchronization in the abstract, as they exist in any operating system kernel. The <a href="../ch10/">next chapter</a> details the specific mechanisms and interfaces that the Linux kernel provides to solve synchronization issues and prevent race conditions.</p>
<h3 id="critical-regions-and-race-conditions">Critical Regions and Race Conditions</h3>
<ul>
<li>Code paths that access and manipulate shared data are called <a href="https://en.wikipedia.org/wiki/Critical_section"><strong>critical regions</strong></a> (also called <strong>critical sections</strong>). It is usually unsafe for multiple threads of execution to access the same resource simultaneously.</li>
<li>To prevent concurrent access during critical regions, the programmer must ensure that code executes <a href="https://en.wikipedia.org/wiki/Linearizability"><em>atomically</em></a>, which means that operations complete without interruption as if the entire critical region were one indivisible instruction.</li>
<li>It is a bug if it is possible for two threads of execution to be simultaneously executing within the same critical region. When this occur, it is called a <a href="https://en.wikipedia.org/wiki/Race_condition"><strong>race condition</strong></a>, so-named because the threads raced to get there first. Debugging race conditions is often difficult because they are not easily reproducible.</li>
<li>Ensuring that unsafe concurrency is prevented and that race conditions do not occur is called <a href="https://en.wikipedia.org/wiki/Synchronization_(computer_science)"><strong>synchronization</strong></a>.</li>
</ul>
<h4 id="why-do-we-need-protection">Why Do We Need Protection?</h4>
<p>To best understand the need for synchronization, look at the ubiquity of race conditions.</p>
<p>The first example is a real-world case: an ATM (Automated Teller Machine, called a cash machine).</p>
<p>After the user has asked for a specific amount of money, the cash machine needs to
ensure that the money actually exists in that user's account and deducts the withdrawal from the total funds available. The code would be like:</p>
<div class="codehilite"><pre><span class="kt">int</span> <span class="n">total</span> <span class="o">=</span> <span class="n">get_total_from_account</span><span class="p">();</span> <span class="cm">/* total funds in account */</span>
<span class="kt">int</span> <span class="n">withdrawal</span> <span class="o">=</span> <span class="n">get_withdrawal_amount</span><span class="p">();</span> <span class="cm">/* amount user asked to withdrawal */</span>

<span class="cm">/* check whether the user has enough funds in her account */</span>
<span class="k">if</span> <span class="p">(</span><span class="n">total</span> <span class="o">&lt;</span> <span class="n">withdrawal</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">error</span><span class="p">(</span><span class="err">“</span><span class="n">You</span> <span class="k">do</span> <span class="n">not</span> <span class="n">have</span> <span class="n">that</span> <span class="n">much</span> <span class="n">money</span><span class="o">!</span><span class="err">”</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/* OK, the user has enough money: deduct the withdrawal amount from her total */</span>
<span class="n">total</span> <span class="o">-=</span> <span class="n">withdrawal</span><span class="p">;</span>
<span class="n">update_total_funds</span><span class="p">(</span><span class="n">total</span><span class="p">);</span>

<span class="cm">/* give the user their money */</span>
<span class="n">spit_out_money</span><span class="p">(</span><span class="n">withdrawal</span><span class="p">);</span>
</pre></div>


<p>Assume that another deduction in the user's funds is happening at the same time, which could be: user's spouse is initiating another withdrawal at another ATM, a payee is electronically transferring funds out of the account, or the bank is deducting a fee from the account (as banks these days are so wont to do).</p>
<p>Both systems performing the withdrawal would have similar code as above: first check whether the deduction is possible, then compute the new total funds, and finally execute the physical deduction. Presume that the first deduction is a withdrawal from an ATM for $100 and that the second deduction is the bank applying a fee of $10 because the customer walked into the bank.  Assume the customer has a total of $105 in the bank. Obviously, one of these transactions cannot correctly complete.</p>
<p>Assume that the two transactions are initiated at roughly the same time. Both transactions verify that sufficient funds exist: $105 is more than both $100 and $10, so all is good. Then the withdrawal process subtracts $100 from $105, yielding $5. The fee transaction then does the same, subtracting $10 from $105 and getting $95. The withdrawal process then updates the user's new total available funds to $5. Now the fee transaction also updates the new total, resulting in $95 (free money).</p>
<p>Clearly, financial institutions must ensure that this can never happen. They must lock the account during certain operations, making each transaction atomic with respect to any other transaction. Such transactions must occur in their entirety, without interruption, or not occur at all.</p>
<h4 id="the-single-variable">The Single Variable</h4>
<p>Consider a simple shared resource, a single global integer, and a simple critical region, the operation of merely incrementing it: <code>i++</code>;</p>
<p>This might translate into machine instructions to the computer's processor that resemble the following:</p>
<ul>
<li>Get the current value of <code>i</code> and copy it into a register.</li>
<li>Add one to the value stored in the register.</li>
<li>Write back to memory the new value of <code>i</code>.</li>
</ul>
<p>Assume that there are two threads of execution, both enter this critical region, and the initial value of <code>i</code> is 7. The desired outcome is then similar to the following (with each row representing a unit of time):</p>
<table>
<thead>
<tr>
<th>Thread 1</th>
<th>Thread 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>get <code>i</code> (7)</td>
<td>—</td>
</tr>
<tr>
<td>increment <code>i</code> (7 -&gt; 8)</td>
<td>—</td>
</tr>
<tr>
<td>write back <code>i</code> (8)</td>
<td>—</td>
</tr>
<tr>
<td>—</td>
<td>get <code>i</code> (8)</td>
</tr>
<tr>
<td>—</td>
<td>increment <code>i</code> (8 -&gt; 9)</td>
</tr>
<tr>
<td>—</td>
<td>write back <code>i</code> (9)</td>
</tr>
</tbody>
</table>
<p>As expected, 7 incremented twice is 9.</p>
<p>However, another possible outcome is the following:</p>
<table>
<thead>
<tr>
<th>Thread 1</th>
<th>Thread 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>get <code>i</code> (7)</td>
<td>get <code>i</code> (7)</td>
</tr>
<tr>
<td>increment <code>i</code> (7 -&gt; 8)</td>
<td>—</td>
</tr>
<tr>
<td>—</td>
<td>increment <code>i</code> (7 -&gt; 8)</td>
</tr>
<tr>
<td>write back <code>i</code> (8)</td>
<td>—</td>
</tr>
<tr>
<td>—</td>
<td>write back <code>i</code> (8)</td>
</tr>
</tbody>
</table>
<p>If both threads of execution read the initial value of <code>i</code> before it is incremented, both threads increment and save the same value. As a result, the variable <code>i</code> contains the value 8 when, in fact, it should now contain 9. This is one of the simplest examples of a critical region. The solution is simple. We merely need a way to perform these operations in one indivisible step. Most processors provide an instruction to atomically read, increment, and write back a single variable. Using this atomic instruction, the only possible outcome (or conversely with Thread 2 incrementing <code>i</code> first) is:</p>
<table>
<thead>
<tr>
<th>Thread 1</th>
<th>Thread 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>increment &amp; store <code>i</code> (7 -&gt; 8)</td>
<td>—</td>
</tr>
<tr>
<td>—</td>
<td>increment &amp; store <code>i</code> (8 -&gt; 9)</td>
</tr>
</tbody>
</table>
<p>It would never be possible for the two atomic operations to interleave. The processor would physically ensure that it was impossible. Using such an instruction would alleviate the problem. The kernel provides a set of interfaces that implement these atomic instructions, which are discussed in the next chapter.</p>
<h3 id="locking">Locking</h3>
<p>Assume you have a queue of requests that needs to be serviced and the implementation is a linked list, in which each node represents a request. Two functions manipulate the queue:</p>
<ul>
<li>One function adds a new request to the tail of the queue.</li>
<li>One function removes a request from the head of the queue and service request.</li>
</ul>
<p>Requests are continually being added, removed, and serviced, since various parts of the kernel invoke these two functions. Manipulating the request queues certainly requires multiple instructions. If one thread attempts to read from the queue while another is in the middle of manipulating it, the reading thread will find the queue in an inconsistent state. It should be apparent the sort of damage that could occur if access to the queue could occur concurrently. Often, when the shared resource is a complex data structure, the result of a race condition is corruption of the data structure.</p>
<p>How can you prevent one processor from reading from the queue while another processor is updating it?  Although it is feasible for a particular architecture to implement simple instructions, such as arithmetic and comparison, atomically it is ludicrous for architectures to provide instructions to support the indefinitely sized critical regions that would exist in the example. What is needed is a way of making sure that only one thread manipulates the data structure at a time, a mechanism for preventing access to a resource while another thread of execution is in the marked region.</p>
<p>A lock provides such a mechanism. [p165] Threads hold locks; locks protect data.</p>
<ul>
<li>Whenever there was a new request to add to the queue, the thread would first obtain the lock. Then it could safely add the request to the queue and ultimately release the lock.</li>
<li>When a thread wanted to remove a request from the queue, it would also obtain the lock. Then it could read the request and remove it from the queue. Finally, it would release the lock.</li>
</ul>
<p>Any other access to the queue would similarly need to obtain the lock.  Because the lock can be held by only one thread at a time, only a single thread can manipulate the queue at a time. If a thread comes along while another thread is already updating it, the second thread has to wait for the first to release the lock before it can continue. The lock prevents concurrency and protects the queue from race conditions.</p>
<p>Any code that accesses the queue first needs to obtain the relevant lock. If another thread of execution comes along, the lock prevents concurrency:</p>
<table>
<thead>
<tr>
<th>Thread 1</th>
<th>Thread 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>try to lock the queue</td>
<td>try to lock the queue</td>
</tr>
<tr>
<td>succeeded: acquired lock</td>
<td>failed: waiting...</td>
</tr>
<tr>
<td>access queue...</td>
<td>waiting...</td>
</tr>
<tr>
<td>unlock the queue</td>
<td>waiting...</td>
</tr>
<tr>
<td>...</td>
<td>succeeded: acquired lock</td>
</tr>
<tr>
<td></td>
<td>access queue...</td>
</tr>
<tr>
<td></td>
<td>unlock the queue</td>
</tr>
</tbody>
</table>
<p>Notice that locks are <em>advisory</em> and <em>voluntary</em>. Locks are entirely a programming construct that the programmer must take advantage of. Nothing prevents you from writing code that manipulates the fictional queue without the appropriate lock, but such a practice would eventually result in a race condition and corruption.</p>
<p>Locks come in various shapes and sizes. Linux alone implements a handful of different locking mechanisms. The most significant difference between the various mechanisms is the behavior when the lock is unavailable because another thread already holds it:</p>
<ul>
<li>Some lock variants <a href="https://en.wikipedia.org/wiki/Busy_waiting">busy wait</a> (spin in a tight loop, checking the status of the lock over and over, waiting for the lock to become available)</li>
<li>Other locks put the current task to sleep until the lock becomes available.</li>
</ul>
<p>The next chapter discusses the behavior of the different locks in Linux and their interfaces.</p>
<p>As you may notice, the lock does not solve the problem; it simply shrinks the critical region down to just the lock and unlock code: probably much smaller, but still a potential race. Fortunately, locks are implemented using atomic operations that ensure no race exists. A single instruction can verify whether the key is taken and, if not, seize it. How this is done is architecture-specific, but almost all processors implement an atomic <a href="https://en.wikipedia.org/wiki/Test-and-set"><em>test and set</em></a> instruction that tests the value of an integer and sets it to a new value only if it is zero. A value of zero means unlocked. On the popular x86 architecture, locks are implemented using such a similar instruction called <a href="https://en.wikipedia.org/wiki/Compare-and-swap"><em>compare and exchange</em></a>.</p>
<h4 id="causes-of-concurrency">Causes of Concurrency</h4>
<p>In user-space, programs are scheduled preemptively at the will of the scheduler. Because a process can be preempted at any time and another process can be scheduled onto the processor, a process can be involuntarily preempted in the middle of accessing a critical region. If the newly scheduled process then enters the same critical region (for example, if the two processes manipulate the same shared memory or write to the same file descriptor), a race can occur. The same problem can occur with multiple single-threaded processes sharing files, or within a single program with signals, because signals can occur asynchronously. This type of concurrency in which two things do not actually happen at the same time but interleave with each other is called <em>pseudo-concurrency</em>.</p>
<p>If you have a symmetrical multiprocessing machine, two processes can actually be executed in a critical region at the exact same time. That is called <em>true concurrency</em>. Although the causes and semantics of true versus pseudo concurrency are different, they both result in the same race conditions and require the same sort of protection.</p>
<p>The kernel has similar causes of concurrency:</p>
<ul>
<li><strong>Interrupts</strong>. An interrupt can occur asynchronously at almost any time, interrupting the currently executing code.</li>
<li><strong>Softirqs and tasklets</strong>. The kernel can raise or schedule a softirq or tasklet at almost any time, interrupting the currently executing code.</li>
<li><strong>Kernel preemption</strong>. Because the kernel is preemptive, one task in the kernel can preempt another.</li>
<li><strong>Sleeping and synchronization with user-space</strong>. A task in the kernel can sleep and thus invoke the scheduler, resulting in the running of a new process.</li>
<li><strong> Symmetrical multiprocessing</strong>. Two or more processors can execute kernel code at exactly the same time.</li>
</ul>
<p>Kernel developers need to understand and prepare for these causes of concurrency:</p>
<ul>
<li>It is a major bug if an interrupt occurs in the middle of code that is manipulating a resource and the interrupt handler can access the same resource.</li>
<li>Similarly, it is a bug if kernel code is preemptive while it is accessing a shared resource.</li>
<li>Likewise, it is a bug if code in the kernel sleeps while in the middle of a critical section.</li>
<li>Finally, two processors should never simultaneously access the same piece of data.</li>
</ul>
<p>With a clear picture of what data needs protection, it is not hard to provide the locking to keep the system stable. Rather, the hard part is identifying these conditions and realizing that to prevent concurrency, you need some form of protection.</p>
<h5 id="design-proper-locking-from-the-beginning"><strong>Design proper locking from the beginning</strong> *</h5>
<p>Implementing the actual locking in your code to protect shared data is not difficult, especially when done early on during the design phase of development. The tricky part is identifying the actual shared data and the corresponding critical sections. This is why designing locking into your code from the get-go, and not as an afterthought, is of paramount importance. It can be difficult to go in, ex post, and identify critical regions and retrofit locking into the existing code. The resulting code is often not pretty, either. The takeaway from this is to always design proper locking into your code from the beginning.</p>
<h5 id="definitions-of-concurrency-safe-terms"><strong>Definitions of concurrency safe terms</strong> *</h5>
<ul>
<li>Code that is safe from concurrent access from an interrupt handler is said to be <em>interrupt-safe</em>.</li>
<li>Code that is safe from concurrency on symmetrical multiprocessing machines is <em>SMP-safe</em>.</li>
<li>Code that is safe from concurrency with kernel preemption is <em>preempt-safe</em> (barring a few exceptions, being SMP-safe implies being preempt-safe).</li>
</ul>
<p>The actual mechanisms used to provide synchronization and protect against race conditions in all these cases is covered in the next chapter.</p>
<h4 id="knowing-what-to-protect">Knowing What to Protect</h4>
<p>Identifying what data specifically needs protection is vital. Since any data that can be accessed concurrently almost assuredly needs protection, <u>it is often easier to identify what data does not need protection and work from there:</u></p>
<ul>
<li>Obviously, any data that is local to one particular thread of execution does not need protection, because only that thread can access the data. For example, local automatic variables (and dynamically allocated data structures whose address is stored only on the stack) do not need any sort of locking because they exist solely on the stack of the executing thread.</li>
<li>Likewise, data that is accessed by only a specific task does not require locking (because a process can execute on only one processor at a time).</li>
</ul>
<h5 id="what-does-need-locking"><strong>What does need locking?</strong> *</h5>
<p>Most global kernel data structures require locking. A good rule of thumb is that if another thread of execution can access the data, the data needs some sort of locking; if anyone else can see it, lock it. <u>Remember to lock data, not code.</u></p>
<h5 id="config-options-smp-versus-up"><strong>CONFIG Options: SMP Versus UP</strong> *</h5>
<p>Because the Linux kernel is configurable at compile time, you can tailor the kernel specifically for a given machine:</p>
<ul>
<li>The <code>CONFIG_SMP</code> configure option controls whether the kernel supports SMP. Many locking issues disappear on uniprocessor machines; consequently, when <code>CONFIG_SMP</code> is unset, unnecessary code is not compiled into the kernel image. For example, such configuration enables uniprocessor machines to forego the overhead of spin locks.</li>
<li>The same trick applies to <code>CONFIG_PREEMPT</code> (the configure option enabling kernel preemption).</li>
</ul>
<p>This was an excellent design decision: the kernel maintains one clean source base, and the various locking mechanisms are used as needed. Different combinations of <code>CONFIG_SMP</code> and <code>CONFIG_PREEMPT</code> on different architectures compile in varying lock support.</p>
<p>In your code, provide appropriate protection for the most pessimistic case, SMP with kernel preemption, and all scenarios will be covered.</p>
<p>Ask yourself these questions whenever you write kernel code:</p>
<ul>
<li>Is the data global? Can a thread of execution other than the current one access it?</li>
<li>Is the data shared between process context and interrupt context? Is it shared between two different interrupt handlers?</li>
<li>If a process is preempted while accessing this data, can the newly scheduled process access the same data?</li>
<li>Can the current process sleep (block) on anything? If it does, in what state does that leave any shared data?</li>
<li>What prevents the data from being freed out from under me?</li>
<li>What happens if this function is called again on another processor?</li>
<li>Given the proceeding points, how am I going to ensure that my code is safe from concurrency?</li>
</ul>
<p>In short, nearly all global and shared data in the kernel requires some form of the synchronization methods, discussed in the next chapter.</p>
<h3 id="doubts-and-solution">Doubts and Solution</h3>
<h4 id="verbatim">Verbatim</h4>
<h5 id="p169-on-knowing-what-to-protect"><strong>p169 on "Knowing What to Protect"</strong></h5>
<blockquote>
<p>What prevents the data from being freed out from under me?</p>
</blockquote>
<p><span class="text-danger">Question</span>: What does it mean and why is it one of the questions asked to ensure the code is safe from concurrency.</p>
            </div>
        </div>

        <footer class="col-md-12">
            
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script src="../../js/base.js"></script>
        <script src="../../custom.js"></script>
    </body>
</html>